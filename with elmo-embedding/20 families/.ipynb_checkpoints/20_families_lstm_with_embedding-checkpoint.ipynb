{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '..\\\\..\\\\')\n",
    "\n",
    "import os\n",
    "import data_loader\n",
    "from numpy import trapz\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Dropout, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "malware_data_dir = '../../data/'\n",
    "#saved_model_path = 'saved_model/'\n",
    "#opcode_to_int_path = \"opcodeToInt.txt\"\n",
    "num_unique_opcodes = 30\n",
    "max_opcode_sequence_length = 2000\n",
    "embed_vector_length = 128\n",
    "num_lstm_unit = 16\n",
    "dropout_amt = 0.3\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "test_size= 0.15       # reserve for testing\n",
    "results_path = \"results.txt\"\n",
    "#num_families_to_use = 20\n",
    "\n",
    "shutdown = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " def split_data1(train_data_raw, train_labels_raw):\n",
    "    # Split into training and testing data\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(train_data_raw, train_labels_raw, test_size=test_size)\n",
    "\n",
    "    # Make divisible by batch size\n",
    "    num_data_train = int(len(train_data)/batch_size) * batch_size\n",
    "    num_data_test = int(len(test_data)/batch_size) * batch_size\n",
    "\n",
    "    train_data = train_data[:num_data_train]\n",
    "    train_labels = train_labels[:num_data_train]\n",
    "    test_data = test_data[:num_data_test]\n",
    "    test_labels = test_labels[:num_data_test]\n",
    "\n",
    "#     print(\"train_data shape: {}\".format(train_data.shape))\n",
    "#     print(\"test_data shape: {}\".format(test_data.shape))\n",
    "#     print(\"train_labels shape: {}\".format(train_labels.shape))\n",
    "#     print(\"test_labels shape: {}\".format(test_labels.shape))\n",
    "\n",
    "    return train_data, test_data, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model1(num_families_to_use):\n",
    "    model = Sequential()\n",
    "    model.add(Input(batch_shape=(batch_size, max_opcode_sequence_length), name=\"input\"))\n",
    "    model.add(Embedding(input_dim=num_unique_opcodes+1,\n",
    "                                  output_dim=embed_vector_length,\n",
    "                                  input_length=max_opcode_sequence_length, name=\"embedding\"))\n",
    "    # LSTM 1\n",
    "    model.add(Dropout(dropout_amt, name=\"dropout_1\"))\n",
    "    model.add(LSTM(num_lstm_unit, \n",
    "                   input_shape=(None, max_opcode_sequence_length),\n",
    "                   name=\"lstm1\"))\n",
    "    model.add(Dense(num_families_to_use, activation='softmax', name=\"dense\"))\n",
    "    optimizer = Adam()\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    #model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model():\n",
    "    num_families_to_use_list = [5,10,15,20]\n",
    "    results = {}\n",
    "    \n",
    "    for num_families_to_use in num_families_to_use_list:\n",
    "        print(\"{0} families....\".format(num_families_to_use))\n",
    "        \n",
    "        with open(results_path, 'a') as file:\n",
    "            file.write(str(num_families_to_use) + \"\\n\")\n",
    "            \n",
    "        opcode_to_int_path = \"opcodeToInt_\" + str(num_families_to_use) + \".txt\"\n",
    "        # Get train data\n",
    "        raw_train_data = data_loader.getTrainData(malware_data_dir, \n",
    "                                                  num_families_to_use, \n",
    "                                                  num_unique_opcodes, \n",
    "                                                  max_opcode_sequence_length, \n",
    "                                                  opcode_to_int_path)\n",
    "        # Data preprocessing\n",
    "        family_names = list(raw_train_data.keys())\n",
    "        print(family_names)\n",
    "\n",
    "        # Split opcode family data in individual lists\n",
    "        train_data = list()\n",
    "        for family, data in raw_train_data.items():\n",
    "            train_data.append(data)\n",
    "\n",
    "        # Pad training data to ensure uniformity\n",
    "        padded_train_data = list()\n",
    "        for family_opcodes in train_data:\n",
    "            padded_sequence = pad_sequences(family_opcodes, \n",
    "                                            maxlen=max_opcode_sequence_length)\n",
    "            padded_train_data.append(padded_sequence)\n",
    "\n",
    "        # Concatenate all training data into 1 long list instead of multiple lists\n",
    "        train_data_raw = np.concatenate(padded_train_data)\n",
    "\n",
    "        print(len(train_data))\n",
    "        \n",
    "        # Make labels\n",
    "        train_labels = []\n",
    "        for count, data in enumerate(padded_train_data):\n",
    "            labels_list = np.full(shape=(len(data)), fill_value=count)\n",
    "            train_labels.append(labels_list)\n",
    "\n",
    "        train_labels_raw = np.concatenate(train_labels)\n",
    "        \n",
    "        for i in range(5):\n",
    "            # get train and test data\n",
    "            train_data, test_data, train_labels, test_labels = split_data(train_data_raw, train_labels_raw)\n",
    "\n",
    "            # train model\n",
    "            model_train = create_model(num_families_to_use)\n",
    "            early_stopping = EarlyStopping(monitor='accuracy', verbose=1, patience=2)\n",
    "            history = model_train.fit(x=train_data,\n",
    "                                  y=train_labels,\n",
    "                                  batch_size=batch_size,\n",
    "                                  callbacks = [early_stopping],\n",
    "                                  epochs=num_epochs,)\n",
    "\n",
    "            # evaluate\n",
    "            model_evaluate = create_model(num_families_to_use)\n",
    "            model_evaluate.set_weights(model_train.get_weights())\n",
    "\n",
    "            scores = model_evaluate.evaluate(test_data, test_labels, verbose=0, callbacks = [early_stopping])\n",
    "            accuracy = scores[1]*100\n",
    "            print(\"{0}: {1}\".format(num_families_to_use, accuracy))\n",
    "            results[num_families_to_use] = accuracy\n",
    "\n",
    "            with open(results_path, 'a') as file:\n",
    "                file.write(str(accuracy) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 families....\n",
      "Getting list of paths to training data\n",
      "{'winwebsec': 6862260, 'vundo': 3492760, 'zbot': 3256944, 'hotbar': 2952000, 'renos': 2612858}\n",
      "Loading training data for hotbar\n",
      "Loading training data for renos\n",
      "Loading training data for vundo\n",
      "Loading training data for winwebsec\n",
      "Loading training data for zbot\n",
      "All training data loaded\n",
      "['hotbar', 'renos', 'vundo', 'winwebsec', 'zbot']\n",
      "5\n",
      "Epoch 1/100\n",
      "265/265 [==============================] - 19s 71ms/step - loss: 1.0930 - accuracy: 0.5923\n",
      "Epoch 2/100\n",
      "265/265 [==============================] - 19s 70ms/step - loss: 0.9104 - accuracy: 0.6544\n",
      "Epoch 3/100\n",
      "265/265 [==============================] - 19s 70ms/step - loss: 0.8672 - accuracy: 0.6784\n",
      "Epoch 4/100\n",
      "265/265 [==============================] - 19s 70ms/step - loss: 0.7538 - accuracy: 0.7376\n",
      "Epoch 5/100\n",
      "265/265 [==============================] - 19s 70ms/step - loss: 0.7654 - accuracy: 0.7278\n",
      "Epoch 6/100\n",
      "265/265 [==============================] - 18s 68ms/step - loss: 0.7587 - accuracy: 0.7185\n",
      "Epoch 00006: early stopping\n",
      "5: 75.611412525177\n",
      "Epoch 1/100\n",
      "265/265 [==============================] - 18s 67ms/step - loss: 1.0916 - accuracy: 0.5853\n",
      "Epoch 2/100\n",
      "265/265 [==============================] - 18s 68ms/step - loss: 0.8533 - accuracy: 0.6816\n",
      "Epoch 3/100\n",
      "265/265 [==============================] - 18s 67ms/step - loss: 0.7926 - accuracy: 0.6983\n",
      "Epoch 4/100\n",
      "265/265 [==============================] - 18s 66ms/step - loss: 0.7314 - accuracy: 0.7252\n",
      "Epoch 5/100\n",
      "265/265 [==============================] - 18s 66ms/step - loss: 0.6850 - accuracy: 0.7377\n",
      "Epoch 6/100\n",
      "265/265 [==============================] - 18s 66ms/step - loss: 0.6496 - accuracy: 0.7545\n",
      "Epoch 7/100\n",
      "265/265 [==============================] - 18s 66ms/step - loss: 0.6306 - accuracy: 0.7742\n",
      "Epoch 8/100\n",
      "265/265 [==============================] - 18s 68ms/step - loss: 0.6143 - accuracy: 0.7712\n",
      "Epoch 9/100\n",
      "265/265 [==============================] - 18s 67ms/step - loss: 0.5895 - accuracy: 0.7875\n",
      "Epoch 10/100\n",
      "265/265 [==============================] - 18s 66ms/step - loss: 0.5560 - accuracy: 0.8000\n",
      "Epoch 11/100\n",
      "265/265 [==============================] - 18s 66ms/step - loss: 0.5209 - accuracy: 0.8143\n",
      "Epoch 12/100\n",
      "265/265 [==============================] - 18s 66ms/step - loss: 0.5989 - accuracy: 0.7797\n",
      "Epoch 13/100\n",
      "265/265 [==============================] - 18s 67ms/step - loss: 0.5311 - accuracy: 0.8052\n",
      "Epoch 00013: early stopping\n",
      "5: 81.25\n",
      "Epoch 1/100\n",
      "265/265 [==============================] - 18s 66ms/step - loss: 1.1874 - accuracy: 0.5715\n",
      "Epoch 2/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.8806 - accuracy: 0.6800\n",
      "Epoch 3/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.7860 - accuracy: 0.7097\n",
      "Epoch 4/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.7144 - accuracy: 0.7415\n",
      "Epoch 5/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.8803 - accuracy: 0.6925\n",
      "Epoch 6/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.9140 - accuracy: 0.6682\n",
      "Epoch 00006: early stopping\n",
      "5: 72.89401888847351\n",
      "Epoch 1/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 1.0896 - accuracy: 0.6117\n",
      "Epoch 2/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.8141 - accuracy: 0.7100\n",
      "Epoch 3/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.7642 - accuracy: 0.7239\n",
      "Epoch 4/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.7145 - accuracy: 0.7412\n",
      "Epoch 5/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.7231 - accuracy: 0.7421\n",
      "Epoch 6/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.6789 - accuracy: 0.7541\n",
      "Epoch 7/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.6388 - accuracy: 0.7709\n",
      "Epoch 8/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.7071 - accuracy: 0.7410\n",
      "Epoch 9/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.5946 - accuracy: 0.7842\n",
      "Epoch 10/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.5629 - accuracy: 0.8004\n",
      "Epoch 11/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.5622 - accuracy: 0.7968\n",
      "Epoch 12/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.5426 - accuracy: 0.8017\n",
      "Epoch 13/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.5321 - accuracy: 0.8075\n",
      "Epoch 14/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.5065 - accuracy: 0.8099\n",
      "Epoch 15/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.4813 - accuracy: 0.8203\n",
      "Epoch 16/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.5274 - accuracy: 0.8048\n",
      "Epoch 17/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.4733 - accuracy: 0.8246\n",
      "Epoch 18/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.4553 - accuracy: 0.8283\n",
      "Epoch 19/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.4792 - accuracy: 0.8218\n",
      "Epoch 20/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.4401 - accuracy: 0.8355\n",
      "Epoch 21/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.5048 - accuracy: 0.8179\n",
      "Epoch 22/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.6623 - accuracy: 0.7619\n",
      "Epoch 00022: early stopping\n",
      "5: 77.173912525177\n",
      "Epoch 1/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 1.1130 - accuracy: 0.5829\n",
      "Epoch 2/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.8437 - accuracy: 0.6983\n",
      "Epoch 3/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.7938 - accuracy: 0.7185\n",
      "Epoch 4/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.8194 - accuracy: 0.7079\n",
      "Epoch 5/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.7532 - accuracy: 0.7340\n",
      "Epoch 6/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.7130 - accuracy: 0.7384\n",
      "Epoch 7/100\n",
      "265/265 [==============================] - 17s 64ms/step - loss: 0.6889 - accuracy: 0.7553\n",
      "Epoch 8/100\n",
      "265/265 [==============================] - 17s 65ms/step - loss: 0.6963 - accuracy: 0.74670s - loss: 0.6978 - \n",
      "Epoch 9/100\n",
      "265/265 [==============================] - 17s 64ms/step - loss: 0.8562 - accuracy: 0.6765\n",
      "Epoch 00009: early stopping\n",
      "5: 75.2038061618805\n",
      "10 families....\n",
      "Getting list of paths to training data\n",
      "{'winwebsec': 6862260, 'vundo': 3492760, 'zbot': 3256944, 'hotbar': 2952000, 'renos': 2612858, 'onlinegames': 2554166, 'obfuscator': 2502965, 'bho': 2315982, 'alureon': 2287866, 'zeroaccess': 2238000}\n",
      "Finding out how many opcodes to use per family...\n",
      "2238000\n",
      "Generating opcode to int mapping...\n",
      "File saved, done.\n",
      "Loading training data for hotbar\n",
      "Loading training data for renos\n",
      "Loading training data for vundo\n",
      "Loading training data for winwebsec\n",
      "Loading training data for zbot\n",
      "Loading training data for alureon\n",
      "Loading training data for bho\n",
      "Loading training data for obfuscator\n",
      "Loading training data for onlinegames\n",
      "Loading training data for zeroaccess\n",
      "All training data loaded\n",
      "['hotbar', 'renos', 'vundo', 'winwebsec', 'zbot', 'alureon', 'bho', 'obfuscator', 'onlinegames', 'zeroaccess']\n",
      "10\n",
      "Epoch 1/100\n",
      "430/430 [==============================] - 28s 65ms/step - loss: 1.7772 - accuracy: 0.4155\n",
      "Epoch 2/100\n",
      "430/430 [==============================] - 28s 65ms/step - loss: 1.4843 - accuracy: 0.5142\n",
      "Epoch 3/100\n",
      "430/430 [==============================] - 28s 65ms/step - loss: 1.3759 - accuracy: 0.5380\n",
      "Epoch 4/100\n",
      "430/430 [==============================] - 26s 60ms/step - loss: 1.4424 - accuracy: 0.5261\n",
      "Epoch 5/100\n",
      "430/430 [==============================] - 28s 65ms/step - loss: 1.4791 - accuracy: 0.5031\n",
      "Epoch 00005: early stopping\n",
      "10: 52.541667222976685\n",
      "Epoch 1/100\n",
      "430/430 [==============================] - 28s 65ms/step - loss: 1.7772 - accuracy: 0.4097\n",
      "Epoch 2/100\n",
      "430/430 [==============================] - 28s 65ms/step - loss: 1.5696 - accuracy: 0.4753\n",
      "Epoch 3/100\n",
      "430/430 [==============================] - 28s 65ms/step - loss: 1.5175 - accuracy: 0.5018\n",
      "Epoch 4/100\n",
      "430/430 [==============================] - 28s 65ms/step - loss: 1.5983 - accuracy: 0.46721s\n",
      "Epoch 5/100\n",
      "430/430 [==============================] - 28s 65ms/step - loss: 1.5134 - accuracy: 0.4920\n",
      "Epoch 00005: early stopping\n",
      "10: 53.79166603088379\n",
      "Epoch 1/100\n",
      "430/430 [==============================] - 28s 64ms/step - loss: 1.8124 - accuracy: 0.41710s - loss: 1.8148 - accura\n",
      "Epoch 2/100\n",
      "430/430 [==============================] - 28s 64ms/step - loss: 1.4736 - accuracy: 0.5086\n",
      "Epoch 3/100\n",
      "430/430 [==============================] - 28s 64ms/step - loss: 1.4484 - accuracy: 0.5156\n",
      "Epoch 4/100\n",
      "430/430 [==============================] - 28s 64ms/step - loss: 1.3676 - accuracy: 0.5341\n",
      "Epoch 5/100\n",
      "430/430 [==============================] - 28s 64ms/step - loss: 1.3349 - accuracy: 0.5533\n",
      "Epoch 6/100\n",
      "430/430 [==============================] - 28s 64ms/step - loss: 1.3671 - accuracy: 0.5445\n",
      "Epoch 7/100\n",
      "430/430 [==============================] - 28s 64ms/step - loss: 1.2923 - accuracy: 0.5669\n",
      "Epoch 8/100\n",
      "430/430 [==============================] - 28s 64ms/step - loss: 1.2470 - accuracy: 0.5786\n",
      "Epoch 9/100\n",
      "430/430 [==============================] - 28s 64ms/step - loss: 1.2457 - accuracy: 0.5828\n",
      "Epoch 10/100\n",
      "430/430 [==============================] - 28s 64ms/step - loss: 1.1971 - accuracy: 0.5981\n",
      "Epoch 11/100\n",
      "430/430 [==============================] - 28s 64ms/step - loss: 1.2203 - accuracy: 0.5903\n",
      "Epoch 12/100\n",
      "430/430 [==============================] - 28s 64ms/step - loss: 1.1198 - accuracy: 0.6138\n",
      "Epoch 13/100\n",
      "430/430 [==============================] - 28s 64ms/step - loss: 1.0818 - accuracy: 0.62980s - los\n",
      "Epoch 14/100\n",
      "430/430 [==============================] - 29s 67ms/step - loss: 1.2776 - accuracy: 0.5844\n",
      "Epoch 15/100\n",
      "430/430 [==============================] - 28s 66ms/step - loss: 1.1364 - accuracy: 0.6118\n",
      "Epoch 00015: early stopping\n",
      "10: 62.87500262260437\n",
      "Epoch 1/100\n",
      "430/430 [==============================] - 28s 65ms/step - loss: 1.7474 - accuracy: 0.4314\n",
      "Epoch 2/100\n",
      "430/430 [==============================] - 28s 65ms/step - loss: 1.4655 - accuracy: 0.5219\n",
      "Epoch 3/100\n",
      "430/430 [==============================] - 28s 65ms/step - loss: 1.5921 - accuracy: 0.4721\n",
      "Epoch 4/100\n",
      "430/430 [==============================] - 28s 65ms/step - loss: 1.5057 - accuracy: 0.4773\n",
      "Epoch 00004: early stopping\n",
      "10: 48.79166781902313\n",
      "Epoch 1/100\n",
      "430/430 [==============================] - 28s 65ms/step - loss: 1.7989 - accuracy: 0.3987\n",
      "Epoch 2/100\n",
      "430/430 [==============================] - 28s 65ms/step - loss: 1.5266 - accuracy: 0.4858\n",
      "Epoch 3/100\n",
      "430/430 [==============================] - 28s 65ms/step - loss: 1.5103 - accuracy: 0.5039\n",
      "Epoch 4/100\n",
      "430/430 [==============================] - 28s 65ms/step - loss: 1.4733 - accuracy: 0.5001\n",
      "Epoch 5/100\n",
      "430/430 [==============================] - 28s 65ms/step - loss: 1.4445 - accuracy: 0.5218\n",
      "Epoch 6/100\n",
      "430/430 [==============================] - 28s 65ms/step - loss: 1.3841 - accuracy: 0.5421\n",
      "Epoch 7/100\n",
      "430/430 [==============================] - 28s 65ms/step - loss: 1.3143 - accuracy: 0.5609\n",
      "Epoch 8/100\n",
      "430/430 [==============================] - 28s 65ms/step - loss: 1.3508 - accuracy: 0.5451\n",
      "Epoch 9/100\n",
      "430/430 [==============================] - 28s 65ms/step - loss: 1.2781 - accuracy: 0.5693\n",
      "Epoch 10/100\n",
      "430/430 [==============================] - 28s 65ms/step - loss: 1.3101 - accuracy: 0.5556\n",
      "Epoch 11/100\n",
      "430/430 [==============================] - 28s 65ms/step - loss: 1.3406 - accuracy: 0.5461\n",
      "Epoch 00011: early stopping\n",
      "10: 56.04166388511658\n",
      "15 families....\n",
      "Getting list of paths to training data\n",
      "{'winwebsec': 6862260, 'vundo': 3492760, 'zbot': 3256944, 'hotbar': 2952000, 'renos': 2612858, 'onlinegames': 2554166, 'obfuscator': 2502965, 'bho': 2315982, 'alureon': 2287866, 'zeroaccess': 2238000, 'delfinject': 2167855, 'startpage': 2164599, 'adload': 2088000, 'fakerean': 2082110, 'cycbot': 2058000}\n",
      "Finding out how many opcodes to use per family...\n",
      "2058000\n",
      "Generating opcode to int mapping...\n",
      "File saved, done.\n",
      "Loading training data for hotbar\n",
      "Loading training data for renos\n",
      "Loading training data for vundo\n",
      "Loading training data for winwebsec\n",
      "Loading training data for zbot\n",
      "Loading training data for alureon\n",
      "Loading training data for bho\n",
      "Loading training data for obfuscator\n",
      "Loading training data for onlinegames\n",
      "Loading training data for zeroaccess\n",
      "Loading training data for adload\n",
      "Loading training data for cycbot\n",
      "Loading training data for delfinject\n",
      "Loading training data for fakerean\n",
      "Loading training data for startpage\n",
      "All training data loaded\n",
      "['hotbar', 'renos', 'vundo', 'winwebsec', 'zbot', 'alureon', 'bho', 'obfuscator', 'onlinegames', 'zeroaccess', 'adload', 'cycbot', 'delfinject', 'fakerean', 'startpage']\n",
      "15\n",
      "Epoch 1/100\n",
      "571/571 [==============================] - 37s 66ms/step - loss: 2.1084 - accuracy: 0.3599\n",
      "Epoch 2/100\n",
      "571/571 [==============================] - 38s 66ms/step - loss: 1.7459 - accuracy: 0.4731\n",
      "Epoch 3/100\n",
      "571/571 [==============================] - 37s 66ms/step - loss: 1.6694 - accuracy: 0.4870\n",
      "Epoch 4/100\n",
      "571/571 [==============================] - 38s 66ms/step - loss: 1.6134 - accuracy: 0.4982\n",
      "Epoch 5/100\n",
      "571/571 [==============================] - 38s 66ms/step - loss: 1.5924 - accuracy: 0.5065\n",
      "Epoch 6/100\n",
      "571/571 [==============================] - 38s 66ms/step - loss: 1.5168 - accuracy: 0.5325\n",
      "Epoch 7/100\n",
      "571/571 [==============================] - 38s 66ms/step - loss: 1.5917 - accuracy: 0.5192\n",
      "Epoch 8/100\n",
      "571/571 [==============================] - 38s 66ms/step - loss: 1.5641 - accuracy: 0.5224\n",
      "Epoch 00008: early stopping\n",
      "15: 51.93750262260437\n",
      "Epoch 1/100\n",
      "571/571 [==============================] - 38s 66ms/step - loss: 2.0557 - accuracy: 0.3652\n",
      "Epoch 2/100\n",
      "571/571 [==============================] - 38s 66ms/step - loss: 1.7453 - accuracy: 0.4647\n",
      "Epoch 3/100\n",
      "571/571 [==============================] - 38s 66ms/step - loss: 1.6294 - accuracy: 0.49260s - loss: 1.6284 - accura\n",
      "Epoch 4/100\n",
      "571/571 [==============================] - 38s 66ms/step - loss: 1.5850 - accuracy: 0.5016\n",
      "Epoch 5/100\n",
      "571/571 [==============================] - 38s 66ms/step - loss: 1.8047 - accuracy: 0.4492\n",
      "Epoch 6/100\n",
      "571/571 [==============================] - 38s 66ms/step - loss: 1.6937 - accuracy: 0.4766\n",
      "Epoch 00006: early stopping\n",
      "15: 50.90625286102295\n",
      "Epoch 1/100\n",
      "571/571 [==============================] - 38s 66ms/step - loss: 2.0859 - accuracy: 0.3638\n",
      "Epoch 2/100\n",
      "571/571 [==============================] - 38s 66ms/step - loss: 1.7538 - accuracy: 0.4571\n",
      "Epoch 3/100\n",
      "571/571 [==============================] - 38s 66ms/step - loss: 1.6895 - accuracy: 0.4771\n",
      "Epoch 4/100\n",
      "571/571 [==============================] - 38s 66ms/step - loss: 1.5999 - accuracy: 0.5063\n",
      "Epoch 5/100\n",
      "571/571 [==============================] - 37s 66ms/step - loss: 1.5463 - accuracy: 0.5138\n",
      "Epoch 6/100\n",
      "571/571 [==============================] - 37s 66ms/step - loss: 1.4861 - accuracy: 0.5299\n",
      "Epoch 7/100\n",
      "571/571 [==============================] - 38s 66ms/step - loss: 1.5240 - accuracy: 0.5238\n",
      "Epoch 8/100\n",
      "571/571 [==============================] - 38s 66ms/step - loss: 1.4872 - accuracy: 0.5418\n",
      "Epoch 9/100\n",
      "571/571 [==============================] - 37s 66ms/step - loss: 1.4162 - accuracy: 0.5570\n",
      "Epoch 10/100\n",
      "571/571 [==============================] - 38s 66ms/step - loss: 1.4912 - accuracy: 0.5345\n",
      "Epoch 11/100\n",
      "571/571 [==============================] - 38s 66ms/step - loss: 1.4766 - accuracy: 0.5343\n",
      "Epoch 00011: early stopping\n",
      "15: 54.56249713897705\n",
      "Epoch 1/100\n",
      "571/571 [==============================] - 37s 65ms/step - loss: 2.1530 - accuracy: 0.3618\n",
      "Epoch 2/100\n",
      "571/571 [==============================] - 37s 65ms/step - loss: 1.7501 - accuracy: 0.4677\n",
      "Epoch 3/100\n",
      "571/571 [==============================] - 37s 65ms/step - loss: 1.7008 - accuracy: 0.4842\n",
      "Epoch 4/100\n",
      "571/571 [==============================] - 37s 65ms/step - loss: 1.5749 - accuracy: 0.5189\n",
      "Epoch 5/100\n",
      "571/571 [==============================] - 37s 65ms/step - loss: 1.5515 - accuracy: 0.5195\n",
      "Epoch 6/100\n",
      "571/571 [==============================] - 37s 65ms/step - loss: 1.5296 - accuracy: 0.5189\n",
      "Epoch 7/100\n",
      "571/571 [==============================] - 37s 65ms/step - loss: 1.4729 - accuracy: 0.5468\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "571/571 [==============================] - 37s 65ms/step - loss: 1.5004 - accuracy: 0.5421\n",
      "Epoch 9/100\n",
      "571/571 [==============================] - 37s 65ms/step - loss: 1.4295 - accuracy: 0.5572\n",
      "Epoch 10/100\n",
      "571/571 [==============================] - 37s 65ms/step - loss: 1.5312 - accuracy: 0.5402\n",
      "Epoch 11/100\n",
      "571/571 [==============================] - 37s 65ms/step - loss: 1.4124 - accuracy: 0.5589\n",
      "Epoch 12/100\n",
      "571/571 [==============================] - 37s 65ms/step - loss: 1.4852 - accuracy: 0.5368\n",
      "Epoch 13/100\n",
      "571/571 [==============================] - 37s 65ms/step - loss: 1.3938 - accuracy: 0.5624\n",
      "Epoch 14/100\n",
      "571/571 [==============================] - 37s 65ms/step - loss: 1.3748 - accuracy: 0.5695\n",
      "Epoch 15/100\n",
      "571/571 [==============================] - 37s 65ms/step - loss: 1.4458 - accuracy: 0.5516\n",
      "Epoch 16/100\n",
      "571/571 [==============================] - 37s 65ms/step - loss: 1.3349 - accuracy: 0.5844\n",
      "Epoch 17/100\n",
      "571/571 [==============================] - 37s 65ms/step - loss: 1.3328 - accuracy: 0.5828\n",
      "Epoch 18/100\n",
      "571/571 [==============================] - 37s 65ms/step - loss: 1.3758 - accuracy: 0.5664\n",
      "Epoch 00018: early stopping\n",
      "15: 57.31250047683716\n",
      "Epoch 1/100\n",
      "571/571 [==============================] - 37s 66ms/step - loss: 2.2730 - accuracy: 0.3098\n",
      "Epoch 2/100\n",
      "571/571 [==============================] - 37s 66ms/step - loss: 1.8649 - accuracy: 0.4281\n",
      "Epoch 3/100\n",
      "571/571 [==============================] - 37s 66ms/step - loss: 1.7384 - accuracy: 0.4626\n",
      "Epoch 4/100\n",
      "571/571 [==============================] - 37s 66ms/step - loss: 1.7594 - accuracy: 0.4481\n",
      "Epoch 5/100\n",
      "571/571 [==============================] - 37s 66ms/step - loss: 1.7548 - accuracy: 0.4545\n",
      "Epoch 00005: early stopping\n",
      "15: 47.68750071525574\n",
      "20 families....\n",
      "Getting list of paths to training data\n",
      "{'winwebsec': 6862260, 'vundo': 3492760, 'zbot': 3256944, 'hotbar': 2952000, 'renos': 2612858, 'onlinegames': 2554166, 'obfuscator': 2502965, 'bho': 2315982, 'alureon': 2287866, 'zeroaccess': 2238000, 'delfinject': 2167855, 'startpage': 2164599, 'adload': 2088000, 'fakerean': 2082110, 'cycbot': 2058000, 'vobfus': 1848000, 'lolyda': 1830000, 'ceeinject': 1725371, 'agent': 1625496, 'rbot': 1623452}\n",
      "Finding out how many opcodes to use per family...\n",
      "1623452\n",
      "Generating opcode to int mapping...\n",
      "File saved, done.\n",
      "Loading training data for hotbar\n",
      "Loading training data for renos\n",
      "Loading training data for vundo\n",
      "Loading training data for winwebsec\n",
      "Loading training data for zbot\n",
      "Loading training data for alureon\n",
      "Loading training data for bho\n",
      "Loading training data for obfuscator\n",
      "Loading training data for onlinegames\n",
      "Loading training data for zeroaccess\n",
      "Loading training data for adload\n",
      "Loading training data for cycbot\n",
      "Loading training data for delfinject\n",
      "Loading training data for fakerean\n",
      "Loading training data for startpage\n",
      "Loading training data for agent\n",
      "Loading training data for ceeinject\n",
      "Loading training data for lolyda\n",
      "Loading training data for rbot\n",
      "Loading training data for vobfus\n",
      "All training data loaded\n",
      "['hotbar', 'renos', 'vundo', 'winwebsec', 'zbot', 'alureon', 'bho', 'obfuscator', 'onlinegames', 'zeroaccess', 'adload', 'cycbot', 'delfinject', 'fakerean', 'startpage', 'agent', 'ceeinject', 'lolyda', 'rbot', 'vobfus']\n",
      "20\n",
      "Epoch 1/100\n",
      "687/687 [==============================] - 46s 67ms/step - loss: 2.4506 - accuracy: 0.2993\n",
      "Epoch 2/100\n",
      "687/687 [==============================] - 48s 70ms/step - loss: 2.0827 - accuracy: 0.3958\n",
      "Epoch 3/100\n",
      "687/687 [==============================] - 48s 70ms/step - loss: 2.0594 - accuracy: 0.4095\n",
      "Epoch 4/100\n",
      "687/687 [==============================] - 48s 70ms/step - loss: 1.8897 - accuracy: 0.4534\n",
      "Epoch 5/100\n",
      "687/687 [==============================] - 47s 69ms/step - loss: 1.8535 - accuracy: 0.4543\n",
      "Epoch 6/100\n",
      "687/687 [==============================] - 48s 69ms/step - loss: 1.7983 - accuracy: 0.4671\n",
      "Epoch 7/100\n",
      "687/687 [==============================] - 47s 68ms/step - loss: 1.7639 - accuracy: 0.4850\n",
      "Epoch 8/100\n",
      "687/687 [==============================] - 48s 69ms/step - loss: 1.8567 - accuracy: 0.4624\n",
      "Epoch 9/100\n",
      "687/687 [==============================] - 48s 70ms/step - loss: 1.9107 - accuracy: 0.4388\n",
      "Epoch 00009: early stopping\n",
      "20: 45.97107470035553\n",
      "Epoch 1/100\n",
      "687/687 [==============================] - 48s 70ms/step - loss: 2.3676 - accuracy: 0.3097\n",
      "Epoch 2/100\n",
      "687/687 [==============================] - 48s 71ms/step - loss: 2.1558 - accuracy: 0.3703\n",
      "Epoch 3/100\n",
      "687/687 [==============================] - 47s 69ms/step - loss: 1.9599 - accuracy: 0.4203\n",
      "Epoch 4/100\n",
      "687/687 [==============================] - 47s 68ms/step - loss: 1.9454 - accuracy: 0.4257\n",
      "Epoch 5/100\n",
      "687/687 [==============================] - 46s 68ms/step - loss: 1.8576 - accuracy: 0.4469\n",
      "Epoch 6/100\n",
      "687/687 [==============================] - 47s 69ms/step - loss: 1.7959 - accuracy: 0.4638\n",
      "Epoch 7/100\n",
      "687/687 [==============================] - 47s 69ms/step - loss: 1.8670 - accuracy: 0.4545\n",
      "Epoch 8/100\n",
      "687/687 [==============================] - 48s 70ms/step - loss: 1.8433 - accuracy: 0.4719\n",
      "Epoch 9/100\n",
      "385/687 [===============>..............] - ETA: 21s - loss: 1.7664 - accuracy: 0.4870"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-d8af0fd933a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-9648688bad8c>\u001b[0m in \u001b[0;36mrun_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mmodel_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_families_to_use\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mearly_stopping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             history = model_train.fit(x=train_data,\n\u001b[0m\u001b[0;32m     55\u001b[0m                                   \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                                   \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1842\u001b[0m     \"\"\"\n\u001b[1;32m-> 1843\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1923\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting list of paths to training data\n",
      "{'winwebsec': 6862260, 'vundo': 3492760, 'zbot': 3256944, 'hotbar': 2952000, 'renos': 2612858, 'onlinegames': 2554166, 'obfuscator': 2502965, 'bho': 2315982, 'alureon': 2287866, 'zeroaccess': 2238000, 'delfinject': 2167855, 'startpage': 2164599, 'adload': 2088000, 'fakerean': 2082110, 'cycbot': 2058000, 'vobfus': 1848000, 'lolyda': 1830000, 'ceeinject': 1725371, 'agent': 1625496, 'rbot': 1623452}\n",
      "Finding out how many opcodes to use per family...\n",
      "1623452\n",
      "Generating opcode to int mapping...\n",
      "File saved, done.\n",
      "Loading training data for hotbar\n",
      "Loading training data for renos\n",
      "Loading training data for vundo\n",
      "Loading training data for winwebsec\n",
      "Loading training data for zbot\n",
      "Loading training data for alureon\n",
      "Loading training data for bho\n",
      "Loading training data for obfuscator\n",
      "Loading training data for onlinegames\n",
      "Loading training data for zeroaccess\n",
      "Loading training data for adload\n",
      "Loading training data for cycbot\n",
      "Loading training data for delfinject\n",
      "Loading training data for fakerean\n",
      "Loading training data for startpage\n",
      "Loading training data for agent\n",
      "Loading training data for ceeinject\n",
      "Loading training data for lolyda\n",
      "Loading training data for rbot\n",
      "Loading training data for vobfus\n",
      "All training data loaded\n"
     ]
    }
   ],
   "source": [
    "raw_train_data = data_loader.getTrainData(malware_data_dir, \n",
    "                                          num_families_to_use, \n",
    "                                          num_unique_opcodes, \n",
    "                                          max_opcode_sequence_length, \n",
    "                                          opcode_to_int_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hotbar', 'renos', 'vundo', 'winwebsec', 'zbot', 'alureon', 'bho', 'obfuscator', 'onlinegames', 'zeroaccess', 'adload', 'cycbot', 'delfinject', 'fakerean', 'startpage', 'agent', 'ceeinject', 'lolyda', 'rbot', 'vobfus']\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "family_names = list(raw_train_data.keys())\n",
    "print(family_names)\n",
    "\n",
    "# Split opcode family data in individual lists\n",
    "train_data = list()\n",
    "for family, data in raw_train_data.items():\n",
    "    train_data.append(data)\n",
    "    \n",
    "# Pad training data to ensure uniformity\n",
    "padded_train_data = list()\n",
    "for family_opcodes in train_data:\n",
    "    padded_sequence = pad_sequences(family_opcodes, \n",
    "                                    maxlen=max_opcode_sequence_length)\n",
    "    padded_train_data.append(padded_sequence)\n",
    "    \n",
    "# Concatenate all training data into 1 long list instead of multiple lists\n",
    "train_data_raw = np.concatenate(padded_train_data)\n",
    "\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "\n",
    "for count, data in enumerate(padded_train_data):\n",
    "    labels_list = np.full(shape=(len(data)), fill_value=count)\n",
    "    train_labels.append(labels_list)\n",
    "\n",
    "train_labels_raw = np.concatenate(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(train_data_raw, train_labels_raw):\n",
    "    # Split into training and testing data\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(train_data_raw, train_labels_raw, test_size=test_size)\n",
    "\n",
    "    # Make divisible by batch size\n",
    "    num_data_train = int(len(train_data)/batch_size) * batch_size\n",
    "    num_data_test = int(len(test_data)/batch_size) * batch_size\n",
    "\n",
    "    train_data = train_data[:num_data_train]\n",
    "    train_labels = train_labels[:num_data_train]\n",
    "    test_data = test_data[:num_data_test]\n",
    "    test_labels = test_labels[:num_data_test]\n",
    "    \n",
    "#     print(\"train_data shape: {}\".format(train_data.shape))\n",
    "#     print(\"test_data shape: {}\".format(test_data.shape))\n",
    "#     print(\"train_labels shape: {}\".format(train_labels.shape))\n",
    "#     print(\"test_labels shape: {}\".format(test_labels.shape))\n",
    "    \n",
    "    return train_data, test_data, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Input(batch_shape=(batch_size, max_opcode_sequence_length), name=\"input\"))\n",
    "    model.add(Embedding(input_dim=num_unique_opcodes+1,\n",
    "                                  output_dim=embed_vector_length,\n",
    "                                  input_length=max_opcode_sequence_length, name=\"embedding\"))\n",
    "    # LSTM 1\n",
    "    model.add(Dropout(dropout_amt, name=\"dropout_1\"))\n",
    "    model.add(LSTM(num_lstm_unit, \n",
    "                   input_shape=(None, max_opcode_sequence_length),\n",
    "                   name=\"lstm1\"))\n",
    "#     # LSTM 2\n",
    "#     model.add(Dropout(dropout_amt, name=\"dropout_2\"))\n",
    "#     model.add(LSTM(num_lstm_unit*2, \n",
    "#                    return_sequences=True,\n",
    "#                    name=\"lstm2\"))\n",
    "#     # LSTM 3\n",
    "#     model.add(Dropout(dropout_amt, name=\"dropout_3\"))\n",
    "#     model.add(LSTM(num_lstm_unit, \n",
    "#                                    name=\"lstm3\"))\n",
    "    model.add(Dense(num_families_to_use, activation='softmax', name=\"dense\"))\n",
    "    optimizer = Adam()\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    #model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "687/687 [==============================] - 48s 69ms/step - loss: 2.4105 - accuracy: 0.2950\n",
      "Epoch 2/100\n",
      "687/687 [==============================] - 46s 68ms/step - loss: 2.0392 - accuracy: 0.4087\n",
      "Epoch 3/100\n",
      "687/687 [==============================] - 47s 68ms/step - loss: 1.9072 - accuracy: 0.4486\n",
      "Epoch 4/100\n",
      "687/687 [==============================] - 46s 68ms/step - loss: 1.8135 - accuracy: 0.4735\n",
      "Epoch 5/100\n",
      "687/687 [==============================] - 46s 67ms/step - loss: 1.7784 - accuracy: 0.4808\n",
      "Epoch 6/100\n",
      "687/687 [==============================] - 47s 68ms/step - loss: 1.7237 - accuracy: 0.4963\n",
      "Epoch 7/100\n",
      "687/687 [==============================] - 46s 67ms/step - loss: 1.9222 - accuracy: 0.4390\n",
      "Epoch 8/100\n",
      "687/687 [==============================] - 45s 66ms/step - loss: 1.8078 - accuracy: 0.4730\n",
      "Epoch 00008: early stopping\n",
      "49.793389439582825\n",
      "Epoch 1/100\n",
      "687/687 [==============================] - 47s 69ms/step - loss: 2.3920 - accuracy: 0.3156\n",
      "Epoch 2/100\n",
      "687/687 [==============================] - 46s 68ms/step - loss: 2.0277 - accuracy: 0.4189\n",
      "Epoch 3/100\n",
      "687/687 [==============================] - 46s 67ms/step - loss: 1.9509 - accuracy: 0.4347\n",
      "Epoch 4/100\n",
      "687/687 [==============================] - 46s 67ms/step - loss: 1.8401 - accuracy: 0.4619\n",
      "Epoch 5/100\n",
      "687/687 [==============================] - 47s 68ms/step - loss: 1.8095 - accuracy: 0.4731\n",
      "Epoch 6/100\n",
      "687/687 [==============================] - 47s 68ms/step - loss: 1.7995 - accuracy: 0.4757\n",
      "Epoch 7/100\n",
      "687/687 [==============================] - 46s 66ms/step - loss: 1.8751 - accuracy: 0.4480\n",
      "Epoch 8/100\n",
      "687/687 [==============================] - 45s 66ms/step - loss: 1.9130 - accuracy: 0.4510\n",
      "Epoch 00008: early stopping\n",
      "48.063015937805176\n",
      "Epoch 1/100\n",
      "687/687 [==============================] - 47s 68ms/step - loss: 2.4562 - accuracy: 0.2921\n",
      "Epoch 2/100\n",
      "687/687 [==============================] - 46s 67ms/step - loss: 2.1132 - accuracy: 0.3839\n",
      "Epoch 3/100\n",
      "687/687 [==============================] - 46s 67ms/step - loss: 1.9638 - accuracy: 0.4233\n",
      "Epoch 4/100\n",
      "687/687 [==============================] - 45s 66ms/step - loss: 1.8679 - accuracy: 0.4505\n",
      "Epoch 5/100\n",
      "687/687 [==============================] - 46s 67ms/step - loss: 1.8904 - accuracy: 0.4484\n",
      "Epoch 6/100\n",
      "687/687 [==============================] - 46s 67ms/step - loss: 1.7840 - accuracy: 0.4755\n",
      "Epoch 7/100\n",
      "687/687 [==============================] - 46s 67ms/step - loss: 1.7204 - accuracy: 0.4990\n",
      "Epoch 8/100\n",
      "687/687 [==============================] - 46s 67ms/step - loss: 1.7461 - accuracy: 0.4888\n",
      "Epoch 9/100\n",
      "687/687 [==============================] - 46s 67ms/step - loss: 1.8794 - accuracy: 0.4518\n",
      "Epoch 00009: early stopping\n",
      "45.17045319080353\n",
      "Epoch 1/100\n",
      "687/687 [==============================] - 47s 69ms/step - loss: 2.4216 - accuracy: 0.3046\n",
      "Epoch 2/100\n",
      "687/687 [==============================] - 49s 71ms/step - loss: 2.0679 - accuracy: 0.40210s - loss: 2.0690 - accu\n",
      "Epoch 3/100\n",
      "687/687 [==============================] - 48s 69ms/step - loss: 2.0109 - accuracy: 0.4202\n",
      "Epoch 4/100\n",
      "687/687 [==============================] - 47s 69ms/step - loss: 1.9884 - accuracy: 0.4276\n",
      "Epoch 5/100\n",
      "687/687 [==============================] - 44s 64ms/step - loss: 1.9499 - accuracy: 0.4396\n",
      "Epoch 6/100\n",
      "687/687 [==============================] - 44s 64ms/step - loss: 2.0153 - accuracy: 0.4170\n",
      "Epoch 7/100\n",
      "687/687 [==============================] - 44s 64ms/step - loss: 1.8804 - accuracy: 0.4551\n",
      "Epoch 8/100\n",
      "687/687 [==============================] - 44s 64ms/step - loss: 1.8992 - accuracy: 0.4560\n",
      "Epoch 9/100\n",
      "687/687 [==============================] - 44s 64ms/step - loss: 1.8038 - accuracy: 0.4719\n",
      "Epoch 10/100\n",
      "687/687 [==============================] - 44s 64ms/step - loss: 1.8002 - accuracy: 0.4717\n",
      "Epoch 11/100\n",
      "687/687 [==============================] - 44s 64ms/step - loss: 1.6745 - accuracy: 0.5080\n",
      "Epoch 12/100\n",
      "687/687 [==============================] - 44s 64ms/step - loss: 1.6591 - accuracy: 0.5148\n",
      "Epoch 13/100\n",
      "687/687 [==============================] - 44s 64ms/step - loss: 1.7320 - accuracy: 0.4992\n",
      "Epoch 14/100\n",
      "687/687 [==============================] - 44s 64ms/step - loss: 1.6338 - accuracy: 0.5296\n",
      "Epoch 15/100\n",
      "687/687 [==============================] - 47s 68ms/step - loss: 1.5539 - accuracy: 0.5479\n",
      "Epoch 16/100\n",
      "687/687 [==============================] - 45s 66ms/step - loss: 1.7770 - accuracy: 0.4979\n",
      "Epoch 17/100\n",
      "687/687 [==============================] - 45s 66ms/step - loss: 1.6388 - accuracy: 0.5226\n",
      "Epoch 00017: early stopping\n",
      "55.24277091026306\n",
      "Epoch 1/100\n",
      "687/687 [==============================] - 46s 67ms/step - loss: 2.3372 - accuracy: 0.3258\n",
      "Epoch 2/100\n",
      "687/687 [==============================] - 46s 67ms/step - loss: 1.9993 - accuracy: 0.4088\n",
      "Epoch 3/100\n",
      "687/687 [==============================] - 47s 68ms/step - loss: 1.9416 - accuracy: 0.4378\n",
      "Epoch 4/100\n",
      "687/687 [==============================] - 46s 68ms/step - loss: 1.8233 - accuracy: 0.4722\n",
      "Epoch 5/100\n",
      "687/687 [==============================] - 46s 68ms/step - loss: 1.7268 - accuracy: 0.4950\n",
      "Epoch 6/100\n",
      "687/687 [==============================] - 47s 68ms/step - loss: 1.9175 - accuracy: 0.4488\n",
      "Epoch 7/100\n",
      "687/687 [==============================] - 46s 67ms/step - loss: 1.8798 - accuracy: 0.4617\n",
      "Epoch 00007: early stopping\n",
      "51.80785059928894\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i in range(5):\n",
    "    # get train and test data\n",
    "    train_data, test_data, train_labels, test_labels = split_data(train_data_raw, train_labels_raw)\n",
    "    \n",
    "    # train model\n",
    "    model_train = create_model()\n",
    "    early_stopping = EarlyStopping(monitor='loss', verbose=1, patience=2)\n",
    "    history = model_train.fit(x=train_data,\n",
    "                          y=train_labels,\n",
    "                          batch_size=batch_size,\n",
    "                          callbacks = [early_stopping],\n",
    "                          epochs=num_epochs,)\n",
    "    \n",
    "    # evaluate\n",
    "    model_evaluate = create_model()\n",
    "    model_evaluate.set_weights(model_train.get_weights())\n",
    "\n",
    "    scores = model_evaluate.evaluate(test_data, test_labels, verbose=0, callbacks = [early_stopping])\n",
    "    accuracy = scores[1]*100\n",
    "    print(accuracy)\n",
    "    results.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.793389439582825\n",
      "48.063015937805176\n",
      "45.17045319080353\n",
      "55.24277091026306\n",
      "51.80785059928894\n"
     ]
    }
   ],
   "source": [
    "for x in results:\n",
    "    print(x)\n",
    "\n",
    "# model_train.save_weights(saved_model_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use History to plot and accuracy throughout training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluate = create_model()\n",
    "model_evaluate.set_weights(model_train.get_weights())\n",
    "\n",
    "scores = model_evaluate.evaluate(test_data, test_labels, verbose=0, callbacks = [early_stopping])\n",
    "accuracy = scores[1]*100\n",
    "print(\"Accuracy: %0.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model From Save and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.load_weights(saved_model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(test_set, test_labels, verbose=0)\n",
    "accuracy = scores[1]*100\n",
    "print(\"Accuracy: %0.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate winwebsec and zbot test data\n",
    "winwebsec_test_data = []\n",
    "zbot_test_data = []\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    if test_labels[i] == 0:\n",
    "        winwebsec_test_data.append(test_set[i])\n",
    "    else:\n",
    "        zbot_test_data.append(test_set[i])\n",
    "        \n",
    "winwebsec_test_data = np.asarray(winwebsec_test_data[:192])\n",
    "zbot_test_data = np.asarray(zbot_test_data[:128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(winwebsec_test_data.shape)\n",
    "print(zbot_test_data.shape)\n",
    "\n",
    "\n",
    "winwebsecY = model.predict(winwebsec_test_data)\n",
    "winwebsecX = [i+1 for i in range(len(winwebsec_test_data))]\n",
    "\n",
    "zbotY = model.predict(zbot_test_data)\n",
    "zbotX = [i+1 for i in range(len(zbot_test_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(100)\n",
    "f = plt.scatter(winwebsecX, winwebsecY, marker='o',\n",
    "                c='darkblue', s=30, label=\"winwebsec\")\n",
    "plt.scatter(zbotX, zbotY, marker='o', c='red', s=30, label=\"zbot\")\n",
    "plt.title(\"Winwebsec vs. Zbot LSTM Prediction Scatter Plot\",\n",
    "          fontsize=18, wrap=True)\n",
    "f.axes.get_xaxis().set_visible(False)\n",
    "plt.ylabel(\"Prediction\", fontsize=15)\n",
    "plt.legend(bbox_to_anchor=(1, 1), loc='upper left', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortByFirstItem(item):\n",
    "    return item[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winwebsecROC = [(data, \"winwebsec\") for data in winwebsecY]\n",
    "zbotROC = [(data, \"zbot\") for data in zbotY]\n",
    "\n",
    "zbotROC.sort(key=sortByFirstItem)\n",
    "winwebsecROC.sort(key=sortByFirstItem)\n",
    "\n",
    "dataROC = zbotROC + winwebsecROC\n",
    "dataROC.sort(key=sortByFirstItem, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_TPR_FPR(thresholdLine, dataROC):\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    TN = 0\n",
    "    FP = 0\n",
    "\n",
    "    for data in dataROC:\n",
    "        yVal = data[0]\n",
    "        family = data[1]\n",
    "\n",
    "        if family == \"winwebsec\":\n",
    "            if yVal < thresholdLine:\n",
    "                TP += 1\n",
    "            else:\n",
    "                FN += 1\n",
    "        elif family == \"zbot\":\n",
    "            if yVal > thresholdLine:\n",
    "                TN += 1\n",
    "            else:\n",
    "                FP += 1\n",
    "    TPR = TP/(TP+FN)\n",
    "    FPR = 1 - (TN/(TN+FP))\n",
    "\n",
    "    return TPR, FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAUC(rocData):\n",
    "    sum = 0\n",
    "\n",
    "    # initialization\n",
    "    prevX = -1\n",
    "    prevY = -1\n",
    "\n",
    "    for points in rocData:\n",
    "        curX = points[0]\n",
    "        curY = points[1]\n",
    "\n",
    "        # Skip for first point\n",
    "        if prevX != -1 and prevY != -1:\n",
    "            # check if rectangle\n",
    "            if prevY == curY:\n",
    "                sum += abs(curX - prevX) * prevY\n",
    "            # check if trapezoid\n",
    "            else:\n",
    "                sum += (curY + prevY) * abs(curX - prevX) * 0.5\n",
    "\n",
    "        prevX = curX\n",
    "        prevY = curY\n",
    "\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocX = list()  # used to plot\n",
    "rocY = list()  # used to plot\n",
    "rocData = list()    # used to calculate AUC\n",
    "\n",
    "for entry in dataROC:\n",
    "    thresholdLine = entry[0]\n",
    "    TPR, FPR = calculate_TPR_FPR(thresholdLine, dataROC)\n",
    "\n",
    "    rocX.append(FPR)\n",
    "    rocY.append(TPR)\n",
    "    rocData.append([FPR, TPR])\n",
    "\n",
    "rocData.sort(key=lambda item: (item[0], item[1]), reverse=True)\n",
    "\n",
    "AUC = round(calculateAUC(rocData), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "plt.figure(200)\n",
    "plt.plot(rocX, rocY, marker=\".\", markersize=8)\n",
    "plt.title(\"Winwebsec vs. Zbot LSTM Log Probability ROC\", fontsize=18)\n",
    "plt.xlabel(\"FPR\", fontsize=15)\n",
    "plt.ylabel(\"TPR\", fontsize=15)\n",
    "plt.grid()\n",
    "plt.text(x=0.75, y=0, s=\"AUC: {0}\".format(AUC), fontsize=14, bbox=props)\n",
    "\n",
    "# show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if shutdown:\n",
    "    os.system('shutdown -s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
